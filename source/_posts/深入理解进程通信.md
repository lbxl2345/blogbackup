title: 深入理解进程通信
tags:
  - linux
  - 操作系统
date: 2017-04-30 11:40:00
---
#### 管道  
管道是进程间的一个单向数据流；进程写入管道的数据都由内核定向到另一个进程，随后另一个进程由此从管道中读取数据，**两个进程必须有亲缘关系，例如兄弟进程、父子进程**。
	
	//经常会使用到的
	$ ls | more
	
管道可以被视为两个“打开的文件”，但它在文件系统中没有响应映象，其原理很简单：`pipe()`系统调用会返回两个FILE文件指针，分别用于读和写，两个进程分别能够读、写管道的内容。
![](https://github.com/lbxl2345/blogbackup/blob/master/source/pics/%E8%BF%9B%E7%A8%8B%E9%80%9A%E4%BF%A1/%E7%AE%A1%E9%81%93.jpg?raw=true)  
管道的结构定义在pipe_fs_i.h中：

	struct pipe_inode_info {
		struct mutex mutex;
		wait_queue_head_t wait;	//FIFO等待队列
		unsigned int nrbufs, curbuf, buffers;
		unsigned int readers;	//读进程编号
		unsigned int writers;	//写进程编号
		unsigned int files;	
		unsigned int waiting_writers;
		unsigned int r_counter;
		unsigned int w_counter;
		struct page *tmp_page;
		struct fasync_struct *fasync_readers;
		struct fasync_struct *fasync_writers;
		struct pipe_buffer *bufs;	//管道缓冲区
		struct user_struct *user;
	};

管道是作为一组VFS对象来实现的，它们用特殊的文件系统pipefs来组织。在`pipe`系统调用发生时，首先会为pipefs分配一个索引节点对象，并且对其进行初始化；随后分别创建一个读文件对象，和一个写文件对象，并对其中的f_ops设置为定义好的操作表地址。随后分配一个dentry，用它把两个文件对象和索引节点对象连接在一起。随后这两个文件描述符就被返回给了用户态进程，于是乎两个进程就能够通过文件描述符来读写数据了。  
 
![](https://github.com/lbxl2345/blogbackup/blob/master/source/pics/%E8%BF%9B%E7%A8%8B%E9%80%9A%E4%BF%A1/pipes.gif?raw=true)  

#### 命名管道FIFO
管道是十分方便的一种结构，不过它存在一个缺点，那就是不能让任意两个进程共享一个管道。命名管道和管道一样，都是借助于文件系统实现的，它遵循“先进先出”的原则，同样依赖于一个内存缓冲区。不过与管道不同的地方在于，管道的索引节点在pipefs文件系统中，而命名管道的节点在系统的目录树里面，所以所有的进程都可以访问命名管道，而且能够用读/写方式来打开。  
命名管道首先需要通过`mknod`或`mkfifo`创建一个FIFO设备文件。一旦创建了FIFO文件，进程就可以用`open`、`write`、`read`等文件操作对FIFO进行操作。在进程使用系统调用`open`时，`init_special_inode()`会把FIFO相关的索引节点对象，进行初始化，并且把`i_fop`设置为定义好的表的地址，并执行`fifo_open()`。  

#### IPC
IPC是（Interprocess Communication）的缩写。它包含信号量、消息队列、共享内存三种方式。IPC在linux中被作为一种**资源**。`semget()`、`msgget()`、`shmget()`都以一个“关键字”作为参数，获得相应的IPC标识符，并且让进程能够通过标识符对资源进行访问。那么在不同的进程中，就可以通过同一关键字来访问IPC。      
每一类IPC都有一个`ipc_ids`数据结构进行管理，其数据结构如下：  

	struct ipc_ids {
		int in_use;				//已经分配的资源数
		unsigned short seq;		//下一个分配位置序号
		struct rw_semaphore rwsem;
		struct idr ipcs_idr;	//用基数树来保存，记录了所有IPC条目
		int next_id;
	};


`kern_ipc_perm`对应一个IPC资源。`ipc_addid()`能够把一个kern_ipc_perm指针，添加到对应`ipc_ids`的基数树当中去。其结构如下，`key`是唯一的标识符。在对应的资源结构题中，都包含有一个kern_ipc_perm指针，它也是管理具体资源的关键所在。  

	struct kern_ipc_perm {
		spinlock_t	lock;
		bool		deleted;
		int		id;	
		key_t		key;		//IPC关键字
		kuid_t		uid;
		kgid_t		gid;
		kuid_t		cuid;
		kgid_t		cgid;
		umode_t		mode;
		unsigned long	seq;	//位置使用序号
		void		*security;
	} ____cacheline_aligned_in_smp;  
	
#### IPC信号量
IPC信号量和内核中的信号量有相似之处，但实际上要比内核信号量复杂：首先IPC信号量可以包含多个信号量值，也就是保护多个独立的数据结构；其次IPC信号量还提供了失效的安全机制，用来处理进程意外死亡的情况。当然，它主要还是作为一种共享资源的访问控制手段，或者用于进程同步，不能传递大量的数据。其定义为`sem_array`。

	struct sem_array {
		struct kern_ipc_perm	sem_perm;	//对应的kern_ipc_perm结构
		time_t			sem_ctime;			/* last change time */
		struct sem		*sem_base;			//第一个sem结构指针
		struct list_head	pending_alter;	//阻塞替换数组的请求队列
											/* that alter the array */
		struct list_head	pending_const;	//阻塞没有替换数组的请求队列
											/* that do not alter semvals */
		struct list_head	list_id;		//用来取消信号量操作
		int			sem_nsems;				//信号量的总数
		int			complex_count;			/* pending complex operations */
		unsigned int		use_global_lock;
	}; 
	
其中，`struct sem`的结构也很简单，除了信号量的值和上次操作的进程pid之外，它还有一个阻塞队列（在最新版本的linux中，这个队列被分成了两个）

	struct sem {
		int	semval;			//信号量的值
		int	sempid;			//上次操作的pid
		spinlock_t	lock;	/* spinlock for fine-grained semtimedop */
		struct list_head	pending_alter;	/* pending operations */
											/* that alter the array */
		struct list_head	pending_const;	/* pending complex operations */
											/* that do not alter semvals */
		time_t	sem_otime;	/* candidate for sem_otime */
	} ____cacheline_aligned_in_smp;
 
具体的组织方式如图所示。每个IPC信号量，都分配了一个挂起请求队列，它标识等待数组中信号量的进程。这也是一个FIFO队列，新的挂起请求都被追加到链表的末尾。  
另一个值得注意的结构是`list_id`，它用来协助信号量的取消工作。它保存了某个进程对信号量所做的所有修改，如果说进程意外的退出了，那么就会把信号量的值恢复成正确的值。  
 ![](https://github.com/lbxl2345/blogbackup/blob/master/source/pics/%E8%BF%9B%E7%A8%8B%E9%80%9A%E4%BF%A1/%E4%BF%A1%E5%8F%B7%E9%87%8F.png?raw=true =600x400) 
 
#### IPC消息
IPC消息保存在一个IPC消息队列中，直到某个进程把它读走为止。它是内核中的消息链表，用队列的形式进行发送和接收，可以保存格式化的数据，并且缓冲区大，读写顺序完全可控。

	struct msg_queue {
		struct kern_ipc_perm q_perm;		//对应的kern_ipc_perm结构
		time_t q_stime;						/* last msgsnd time */
		time_t q_rtime;						/* last msgrcv time */
		time_t q_ctime;						/* last change time */
		unsigned long q_cbytes;				//队列中的字节数
		unsigned long q_qnum;				//队列中的消息数
		unsigned long q_qbytes;				/* max number of bytes on queue */
		pid_t q_lspid;						/* pid of last msgsnd */
		pid_t q_lrpid;						/* last receive pid */
	
		struct list_head q_messages;		//消息链表
		struct list_head q_receivers;		//接收消息的进程链表
		struct list_head q_senders;			//发送消息的进程链表
	};

对于q_messages来说，每条消息都用一个struct `msg_msg`来表示：  

	struct msg_msg {
		struct list_head m_list;
		long m_type;
		size_t m_ts;		/* message text size */
		struct msg_msgseg *next;
		void *security;
		/* the actual message follows immediately */
	};

真正的消息部分紧跟在`msg_msg`的内存区域之后：  

	struct msgbuf {
		__kernel_long_t mtype;          /* type of message */
		char mtext[1];                  /* message text */
	};

整个消息队列的工作机制如图所示。用户可以通过一个整数值来进行标识，这就允许进程有选择的从消息队列中获取消息。只要进程从消息队列中读消息，内核就会把读的消息删除。发送消息和接收消息分别使用`msgsnd`和`msgrcv`函数来完成。

![](https://github.com/lbxl2345/blogbackup/blob/master/source/pics/%E8%BF%9B%E7%A8%8B%E9%80%9A%E4%BF%A1/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97.png?raw=true =600x400)

#### IPC共享内存
共享内存是IPC里面，最快的一种方式。其本质就是一块物理内存同时映射到多个进程各自的进程空间当中。相应的，每个进程都要在自己的地址空间中，增加一个新的内存区，他映射与这个共享内存区相关的页框。共享内存区用`shmid_kernel`来表示：  

	struct shmid_kernel /* private to the kernel */
	{	
		struct kern_ipc_perm	shm_perm;	//kern_ipc_perm数据结构
		struct file		*shm_file;			//共享段的特殊文件
		unsigned long		shm_nattch;
		unsigned long		shm_segsz;
		time_t			shm_atim;
		time_t			shm_dtim;
		time_t			shm_ctim;
		pid_t			shm_cprid;
		pid_t			shm_lprid;
		struct user_struct	*mlock_user;
	
		/* The task created the shm object.  NULL if the task is dead. */
		struct task_struct	*shm_creator;
		struct list_head	shm_clist;	/* list by creator */
	};

`shmid_kernel`中，一个很重要的域就是`shm_file`，因为共享段实际上是一个特殊的文件。我们知道在`vm_area_struct`当中，包含一个`vm_file`，也就是映射文件的域。不过shm文件系统并没有对应到目录树当中去，所以其操作只有mmap。对于共享内存映射来说，会通过`address_space`把页框包含在页高速缓存当中去。    
![](https://github.com/lbxl2345/blogbackup/blob/master/source/pics/%E8%BF%9B%E7%A8%8B%E9%80%9A%E4%BF%A1/%E5%85%B1%E4%BA%AB%E5%86%85%E5%AD%98.png?raw=true =600x400)  

#### Socket
Socket与其它方法的不同之处在于，它能够用于不同机器之间的进程通信。它与网络相关，涉及到具体的协议；作为本机通信时，可以设置对应的参数为`AF_UNIX`或`AF_INET`。注意，这里有两种方法，第一种是依然利用网络socket，把地址设置为localhost，这样通信依然会经过网络协议栈；第二种是利用**Donmain Socket**，不需要经过打包拆包、计算校验等过程，其本质是创建一个socket类型的文件。  
在服务端：应用程序用系统调用`socket`创建一个套接字，并且用系统调用`bind`，将其绑定到IP地址和端口号上去，随后监听这个套接字，并且通过`accept`接收请求。在确定建立请求之后，通过`send`可以与客户端进行交互。    

	server_sockfd = socket(AF_INET, SOCK_STREAM, 0);  
	server_addr.sin_family = AF_INET;				//指定网络套接字  
    server_addr.sin_addr.s_addr = htonl(INADDR_ANY);//接受所有IP地址的连接  
    server_addr.sin_port = htons(9736);				//绑定端口   
    bind(server_sockfd, (struct sockaddr*)&server_addr, sizeof(server_addr));//绑定套接字     
    listen(server_sockfd, 5);监听套接字，建立一个队列
    
    while(1){
    	client_fd = accept(sockfd, (struct sockaddr*)&remote_addr, &sin_size));
    	//创建子进程处理连接
    	if(!fork()){
    		if(send(client_fd, "Hellp, you are connected!", 26, 0) == -1)
    		...
    	}
    }   
    
在客户端：同样用系统调用`socket`创建一个套接字，但是用`connect`函数来尝试建立连接。在连接之后，使用`recv`系统调用接收服务器的信息。  

	sockfd = socket(AF_INET, SOCK_STREAM,0)) == -1;
	
	serv_addr.sin_family = AF_INET;  
    serv_addr.sin_port = htons(SERVPORT);  
    serv_addr.sin_addr = *((struct in_addr*)host->h_addr);  
    bzero(&(serv_addr.sin_zero), 8);  
      
    //面向连接的socket通信要用connect在客户端首先连接  
    if(connect(sockfd, (struct sockaddr *)&serv_addr, sizeof(struct sockaddr)) == -1)  
    {  
        perror("connect 出错！");  
        exit(1);  
    }  
      
    //用于接收服务器的反馈信息  
    if((recvbytes = recv(sockfd, buf, MAXDATASIZE, 0)) == -1)  
    {  
        perror("recv出错！");  
        exit(1);  
    }  

#### 内存映射&信号
内存映射其实就是在两个进程中，同时映射一个文件，然后通过文件中的内容进行通信。    
信号也可以用于进程通信。之前的博文已经详细说明了，不再赘述。  

 
